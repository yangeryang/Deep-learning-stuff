{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Trains a LSTM on the Twitter sentiment classification task.\n",
    "Notes:\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import csv\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocabulary = 5000  # Original 20000.\n",
    "maxlen = 50  # Original 140. cut texts after this number of words (among top max_vocabulary most common words)\n",
    "batch_size = 128  # Original 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_words(words):\n",
    "    # Remove consecutive period symbols\n",
    "    words = re.sub(r\"[\\. ][\\. ]+\", \" . \", words)\n",
    "    # Replace word+comma with word [space] comma\n",
    "    words = re.sub(r\",\", \" , \", words)\n",
    "    # Replace word+parenthesis with word [space] parenthesis.\n",
    "    # E.g., this(is) -> this ( is )\n",
    "    words = re.sub(r\"[\\(\\)]\", \" \\1 \", words)\n",
    "    return words.split()\n",
    "\n",
    "def load_twitter(filename):\n",
    "    labels = []\n",
    "    tweets = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            # Column names = ItemID,Sentiment,SentimentSource,SentimentText\n",
    "            labels.append(int(row['Sentiment']))\n",
    "            words = row['SentimentText'].strip().lower()\n",
    "            tweets.append(preprocess_words(words))\n",
    "    return np.array(tweets), np.array(labels)\n",
    "\n",
    "print('Loading data...')\n",
    "tweets, labels = load_twitter('Sentiment_Analysis_Dataset.csv')\n",
    "print('Loaded', len(labels), 'tweets')\n",
    "print(\"First tweet: {}\".format(' '.join(tweets[0])))\n",
    "print(\"First tweet label: {}\".format(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomize tweets and create training/test sets\n",
    "np.random.seed(1337)\n",
    "rand_idx = np.random.permutation(len(labels))\n",
    "rand_idx[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select last 1000000 tweets as test set for faster training. (originally: 10000)\n",
    "tweets_training = tweets[rand_idx[:-1000000]]\n",
    "labels_training = labels[rand_idx[:-1000000]]\n",
    "tweets_test = tweets[rand_idx[-1000000:]]\n",
    "labels_test = labels[rand_idx[-1000000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tweets):\n",
    "    print('### Example tweets:')\n",
    "    print(' '.join(tweets[0]))\n",
    "    print(' '.join(tweets[1]))\n",
    "    vocab = dict()\n",
    "    for t in tweets:\n",
    "        for word in t:\n",
    "            if word.startswith('@'): # ignore twitter username\n",
    "                continue\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0\n",
    "            vocab[word] += 1\n",
    "    # sort vocabulary by count\n",
    "    vocab = sorted(vocab.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # keep only top max_vocabulary ones\n",
    "    vocab = vocab[:max_vocabulary-1]\n",
    "    vocab.append(('<unk>', 0))\n",
    "    print('### Top 5 vocabs after sorting:')\n",
    "    print(vocab[:5])\n",
    "    return vocab\n",
    "\n",
    "vocabulary = build_vocabulary(tweets_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_vocab(vocabulary):\n",
    "    with open('vocab.txt', 'w') as vf:\n",
    "        for v in vocabulary:\n",
    "            vf.write(v[0])\n",
    "            vf.write('\\t')\n",
    "            vf.write(str(v[1]))\n",
    "            vf.write('\\n')\n",
    "\n",
    "save_vocab(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vocab_index(vocab):\n",
    "    vocab_idx = dict()\n",
    "    v_id = 0\n",
    "    for v in vocab:\n",
    "        vocab_idx[v[0]] = v_id\n",
    "        v_id += 1\n",
    "    return vocab_idx\n",
    "\n",
    "vocab_word_to_id = create_vocab_index(vocabulary)\n",
    "vocab_id_to_word = [(idx,word) for (word,idx) in vocab_word_to_id.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transcode_words(sents, vocab_index):\n",
    "    coded_words = [[vocab_index[w] if w in vocab_index else vocab_index['<unk>'] for w in words ] for words in sents]\n",
    "    return coded_words\n",
    "\n",
    "tweets_training_to_id = transcode_words(tweets_training, vocab_word_to_id)\n",
    "tweets_test_to_id = transcode_words(tweets_test, vocab_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# must transcode from word to word_id first!!\n",
    "print('Pad sequences (samples x time)')\n",
    "tweets_training_to_id_padded = sequence.pad_sequences(tweets_training_to_id, maxlen=maxlen)\n",
    "tweets_test_to_id_padded = sequence.pad_sequences(tweets_test_to_id, maxlen=maxlen)\n",
    "print('features shape:', tweets_training_to_id_padded.shape)\n",
    "# turn label to one-hot\n",
    "labels_training_onehot = to_categorical(labels_training, num_classes=2)\n",
    "labels_test_onehot = to_categorical(labels_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    print('Building model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_vocabulary, 300))\n",
    "    model.add(LSTM(16, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train(model):\n",
    "    print('Train...')\n",
    "    model.fit(tweets_training_to_id_padded, labels_training_onehot,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              validation_data=(tweets_test_to_id_padded, labels_test_onehot))\n",
    "    score, acc = model.evaluate(tweets_test_to_id_padded, labels_test_onehot, batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "tweet_classify_model = build_model()\n",
    "train(tweet_classify_model)\n",
    "tweet_classify_model.save('tweet_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras.models import load_model\n",
    "pre_trained_model = load_model('tweet_model.pkl')\n",
    "pre_trained_model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
